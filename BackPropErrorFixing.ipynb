{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bekzat17/ML_fall_2019/blob/master/BackPropErrorFixing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx0iI_th6zIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "k = 1\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-k * x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    # return sigmoid(x)*(1.0-sigmoid(x))\n",
        "    return x * (1.0 - x)  # CORRECTED\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1.0 - x ** 2\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        self.activation = sigmoid\n",
        "        self.activation_prime = sigmoid_prime\n",
        "        self.output_errors = []\n",
        "\n",
        "        # self.activation = tanh\n",
        "        # self.activation_prime = tanh_prime\n",
        "\n",
        "        # Set weights\n",
        "        self.weights = []\n",
        "        # layers = [2,2,1]\n",
        "        # range of weight values (-1,1)\n",
        "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
        "\n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2 * np.random.random((layers[i - 1] + 1, layers[i] + 1)) - 1\n",
        "            self.weights.append(r)\n",
        "            print(r)\n",
        "        # output layer - random((2+1, 1)) : 3 x 1\n",
        "        r = 2 * np.random.random((layers[i] + 1, layers[i + 1])) - 1\n",
        "        print(r)\n",
        "        self.weights.append(r)\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
        "        # Add column of ones to X\n",
        "        # This is to add the bias unit to the input layer\n",
        "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "        X = np.concatenate((ones.T, X), axis=1)\n",
        "        for k in range(epochs):\n",
        "            i = np.random.randint(X.shape[0])\n",
        "            a = [X[i]]\n",
        "\n",
        "            for l in range(len(self.weights)):\n",
        "                dot_value = np.dot(a[l], self.weights[l])\n",
        "                activation = self.activation(dot_value)\n",
        "                a.append(activation)\n",
        "            # output layer\n",
        "            error = y[i] - a[-1]\n",
        "            deltas = [error * self.activation_prime(a[-1])]\n",
        "            self.output_errors.append(error)  # ADDED\n",
        "            # we have to start at the second to last layer\n",
        "            # (a layer before the output layer)\n",
        "            for l in range(len(a) - 2, 0, -1):\n",
        "                deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_prime(a[l]))\n",
        "\n",
        "            # reverse\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            deltas.reverse()\n",
        "\n",
        "            # backpropagation\n",
        "            # 1. Multiply its output delta and input activation\n",
        "            #    to get the gradient of the weight.\n",
        "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = np.atleast_2d(a[i])\n",
        "                delta = np.atleast_2d(deltas[i])\n",
        "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
        "\n",
        "            if k % 10000 == 0:\n",
        "                print('epochs:', k)\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        a = np.concatenate((np.ones(1).T, np.array(x)))\n",
        "\n",
        "        for l in range(0, len(self.weights)):\n",
        "            a = self.activation(np.dot(a, self.weights[l]))\n",
        "        return a\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2, 2, 1])\n",
        "    X = np.array([[0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "    # X = np.array([[-1, -1],\n",
        "    #               [-1, 1],\n",
        "    #               [1, -1],\n",
        "    #               [1, 1]])\n",
        "    # y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "    for e in X:\n",
        "        print(e, nn.predict(e))\n",
        "\n",
        "\n",
        "x = np.linspace(0, 100000, 100000)\n",
        "error = nn.output_errors\n",
        "plt.plot(x, error)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}